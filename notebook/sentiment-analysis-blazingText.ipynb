{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "256d0a00",
   "metadata": {},
   "source": [
    "# Sentiment Analysis using Blazing Text Algorithm\n",
    "In this exercise we will be using the Sagemaker Blazing Text algorithm which provides highly optimized implementations of the Word2vec and text classification algorithm. Text classification is a Natural Language Processing (NLP) task which can help determine the sentiment of a statement.\n",
    "\n",
    "We will use a public dataset for our training activity. Blazing Text algorithm requires the input to be in a standard format. This format requires a statement and its corresponding label to be in a single line. \n",
    "If your training dataset is across multiple files, then these files have to be concatenanted to create a single file with all the text lines for the algorithm.\n",
    "\n",
    "You can read more about this algorithm [here](https://docs.aws.amazon.com/sagemaker/latest/dg/blazingtext.html).\n",
    "\n",
    "Run each cell and move on to the next cell by pressing ```Shift + Enter```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d9e826",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import section\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "import json\n",
    "import boto3\n",
    "import csv\n",
    "import numpy as np\n",
    "# The sesssion variable will be used to access the default bucket which will host the \n",
    "# training and validation datasets along with output\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "role = get_execution_role()\n",
    "print(\n",
    "    role\n",
    ")  # This is the role that SageMaker would use to leverage AWS resources (S3, CloudWatch) on your behalf\n",
    "\n",
    "bucket = sess.default_bucket()  # Replace with your own bucket name if needed\n",
    "print(bucket)\n",
    "prefix = \"lakehouse\"  # Replace with the prefix under which you want to store the data if needed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ba6bce",
   "metadata": {},
   "source": [
    "## Product Review Dataset\n",
    "In order to train our model we will be using a public dataset. There are several public datasets available that could be used. One such dataset is the [Amazon Product Review dataset](http://jmcauley.ucsd.edu/data/amazon/). \n",
    "\n",
    "We will be using the Clothing, Shoes and Jewelry dataset which has 278,677 reviews.\n",
    "\n",
    "Each line in the product review file is a JSON line with the following attributes:\n",
    "* reviewerID - ID of the reviewer, e.g. A2SUAM1J3GNN3B\n",
    "* asin - ID of the product, e.g. 0000013714\n",
    "* reviewerName - name of the reviewer\n",
    "* helpful - helpfulness rating of the review, e.g. 2/3\n",
    "* **reviewText** - text of the review\n",
    "* **overall** - rating of the product (a value between 1 to 5)\n",
    "* summary - summary of the review\n",
    "* unixReviewTime - time of the review (unix time)\n",
    "* reviewTime - time of the review (raw)\n",
    "\n",
    "The 2 attributes within this dataset that are key to the sentiment analysis are:\n",
    "**reviewText** and **overall**.\n",
    "Lets download and unzip the gunzip file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9793ad93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the gz file, overwrite if the file exists and name the file as amazon_pr.json.gz.\n",
    "!wget -q https://s3.amazonaws.com/ee-assets-prod-us-east-1/modules/ff456f46d8a845799e6ba8fdccc0a4e5/v1/reviews_Clothing_Shoes_and_Jewelry_5.json.gz -O amazon_pr.json.gz\n",
    "\n",
    "# Unzip the Office Product Review file\n",
    "!gzip -fd amazon_pr.json.gz \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d36161",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets view the first 10 lines of the unzipped dataset\n",
    "!head amazon_pr.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10fc7d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import shuffle library to randomly shuffle the dataset to avoid bias\n",
    "from random import shuffle\n",
    "# Import the Natural Language Toolkit(NLTK). \n",
    "# The NLTK data package includes a pre-trained Punkt tokenizer for English.\n",
    "import nltk\n",
    "\n",
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a86308b",
   "metadata": {},
   "source": [
    "## Prepare Data\n",
    "Preparing a dataset is a key step in the Machine Learning process. Preparation of a dataset is unique to the dataset that will be used to train the machine learning model. Some of the reasons data has to be prepared prior to training is to:\n",
    "* Avoid noise - Drop columns that are not relevant to the machine learning problem\n",
    "* Input requirements of Machine Learning algorithms - As we will see for the BlazingText algorithm ahead.\n",
    "* Avoid sparse data problems - Datasets can vary in quality with some having missing data which should be remediated.\n",
    "\n",
    "In our example, we will:\n",
    "* Create a Pandas Dataframe and load the Product Review file made up of JSONLines.\n",
    "* Drop Columns that are not relevant to the business problem\n",
    "* Introduce a new label column based on the value of an existing column. BlazingText algorithm requires the label to be prefixed by **\\_\\_label\\_\\_**\n",
    "* Do a random shuffle to ensure remove bias from the data\n",
    "* Split the training data into 90:10 training and validation dataset\n",
    "\n",
    "**Note:**\n",
    "For ``supervised`` mode, the training/validation file should contain a training sentence per line along with the labels. Labels are words that are prefixed by the string ``__label__``. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10e0349",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "data_df = pd.read_json('amazon_pr.json', lines=True)\n",
    "\n",
    "# Remove unnecessary columns\n",
    "# Retain only the reviewText Column and the Overall column. \n",
    "# The Overall column stores the rating provided by the reviewer on a scale from 1-5\n",
    "\n",
    "data_df=data_df.drop(['reviewerID', 'asin','reviewerName','helpful','summary','unixReviewTime','reviewTime'], axis=1)\n",
    "# Display the top5 rows of the dataframe\n",
    "data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3af2df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if any of the values are NULL\n",
    "# A value of True suggests that are are NULLs. These values can either be set or removed depending \n",
    "# on the number of rows affected.\n",
    "# A value of False suggests that there are no empty values in both dataframe columns\n",
    "data_df.isna().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4d8b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a method label_create that will create a categorisation based on the overall rating field.\n",
    "# You can choose your own categorisation. \n",
    "# For this activity we have followed the below label creation logic:\n",
    "    # BlazingText algorithm requires the label value to be prefixed by \"__label__\"\n",
    "    # If the overall (rating) is 1 or 2 Set the Label to __label__1 (or Negative Sentiment)\n",
    "    # If the overall (rating) is 3 or 4 Set the Label to __label__2 (or Neutral Sentiment)\n",
    "    # If the overall (rating) is 5 Set the Label to __label__3 (or Positive Sentiment)\n",
    "# You could change this logic to create your own text labels(classifications)\n",
    "\n",
    "def label_create(row):\n",
    "    if row['overall'] == 1 or row['overall'] == 2 :\n",
    "        val = '__label__1'\n",
    "    elif row['overall'] == 3 or row['overall'] == 4 :\n",
    "        val = '__label__2'\n",
    "    elif row['overall'] == 5 :\n",
    "        val = '__label__3'\n",
    "    return val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccfc71ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new dataframe column called 'label' and use the label_create method to set values\n",
    "data_df['label'] = data_df.apply(label_create, axis=1)\n",
    "\n",
    "# Drop the overall column from the dataframe as this is now replaced with label column\n",
    "data_df=data_df.drop(['overall'], axis=1)\n",
    "\n",
    "# Change the reviewText to lowercase\n",
    "data_df[\"reviewText\"] = data_df[\"reviewText\"].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52547fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Once done lets look at text classification spread.\n",
    "# For an effective model, the model should ideally be trained on a dataset with adequate representations\n",
    "# from each label\n",
    "\n",
    "data_df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff22de18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# At this stage it is a good idea to shuffle and then split your training dataset \n",
    "# into training and validation dataset.\n",
    "\n",
    "# Use 90:10 split for training:validation\n",
    "fractions = np.array([0.9, 0.1])\n",
    "# shuffle your input\n",
    "data_df = data_df.sample(frac=1) \n",
    "# split into 2 parts\n",
    "train, val = np.array_split(data_df, (fractions[:-1].cumsum() * len(data_df)).astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc472a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first 5 rows of train dataframe.\n",
    "# Notice how the index values have been shuffled (out of order)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44556fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 2 csv files, val (validation) and train(training) to start training the ML model\n",
    "val.to_csv('val.csv', mode='w', sep=' ', columns=['label','reviewText'], index=False, header=False)\n",
    "train.to_csv('train.csv', mode='w', sep=' ', columns=['label','reviewText'], index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293bba74",
   "metadata": {},
   "source": [
    "### Set the Training and Validation channel\n",
    "Once the dataset is shuffled and split, the training file should be uploaded to the train channel and the validation dataset should be uploaded under the validation channel (Using a validation channel is optional)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f59bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Set the channel names\n",
    "train_channel = prefix + \"/train\"\n",
    "validation_channel = prefix + \"/validation\"\n",
    "\n",
    "# Upload the training and validation dataset to the default bucket\n",
    "sess.upload_data(path=\"train.csv\", bucket=bucket, key_prefix=train_channel)\n",
    "sess.upload_data(path=\"val.csv\", bucket=bucket, key_prefix=validation_channel)\n",
    "# Set the location for the training & validation data.\n",
    "s3_train_data = \"s3://{}/{}\".format(bucket, train_channel)\n",
    "s3_validation_data = \"s3://{}/{}\".format(bucket, validation_channel)\n",
    "# Set the location for the output data. This is where the model once generated will be stored.\n",
    "s3_output_location = \"s3://{}/{}/output\".format(bucket, prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c21a6d6",
   "metadata": {},
   "source": [
    "### Fetch Container image\n",
    "Get the container image name for the Sagemaker BlazingText algorithm in the region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d595bd15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the container image name for the Sagemaker BlazingText algorithm\n",
    "region_name = boto3.Session().region_name\n",
    "container = sagemaker.amazon.amazon_estimator.get_image_uri(region_name, \"blazingtext\", \"latest\")\n",
    "print(\"Using SageMaker BlazingText container: {} ({})\".format(container, region_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6609dae9",
   "metadata": {},
   "source": [
    "### Define the resource configuration \n",
    " Define the resource configuration and hyperparameters to perform the text classification using ``supervised`` mode on a ``ml.c4.4xlarge`` instance.\n",
    "\n",
    "The [algorithm documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/blazingtext_hyperparameters.html) for the complete list of hyperparameters. As this is a Text Classification problem, look for Text Classification Hyperparameters.\n",
    "\n",
    "Sagemaker allows performing [hyperparameter tuning](https://docs.aws.amazon.com/sagemaker/latest/dg/blazingtext-tuning.html) to find the best set of hyperparamters for the machine learning problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e0f0ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "bt_model = sagemaker.estimator.Estimator(\n",
    "    container,\n",
    "    role,\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.c4.4xlarge\",\n",
    "    volume_size=30,\n",
    "    max_run=360000,\n",
    "    input_mode=\"File\",\n",
    "    output_path=s3_output_location,\n",
    "    hyperparameters={\n",
    "        \"mode\": \"supervised\",\n",
    "        \"epochs\": 5,\n",
    "        \"min_count\": 2,\n",
    "        \"learning_rate\": 0.05,\n",
    "        \"vector_dim\": 10,\n",
    "        \"early_stopping\": True,\n",
    "        \"patience\": 4,\n",
    "        \"min_epochs\": 5,\n",
    "        \"word_ngrams\": 2,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e48ee2",
   "metadata": {},
   "source": [
    "### Other configurations\n",
    "Set the training inputs and data channels prior to running training the ML model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c387632b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = sagemaker.inputs.TrainingInput(\n",
    "    s3_train_data,\n",
    "    distribution=\"FullyReplicated\",\n",
    "    content_type=\"text/plain\",\n",
    "    s3_data_type=\"S3Prefix\",\n",
    ")\n",
    "validation_data = sagemaker.inputs.TrainingInput(\n",
    "    s3_validation_data,\n",
    "    distribution=\"FullyReplicated\",\n",
    "    content_type=\"text/plain\",\n",
    "    s3_data_type=\"S3Prefix\",\n",
    ")\n",
    "data_channels = {\"train\": train_data, \"validation\": validation_data}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d40c735",
   "metadata": {},
   "source": [
    "### Train the model\n",
    "Once the input channels are defined and the hyperparameters are set the ML training can begin. \n",
    "The ``fit`` method is run to start the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6137c4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "bt_model.fit(inputs=data_channels, logs=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbca6f62",
   "metadata": {},
   "source": [
    "#### Training and Validation Accuracy\n",
    "The training and validation accuracy can be noted in the above run by looking at the ```train_accuracy``` and ```validation_accuracy``` . \n",
    "\n",
    "**Note:**\n",
    "While the quality training dataset plays an important role in increasing the accuracy of the ML model, the performance of the ML model can be improved by performing automatic model tuning with SageMaker. For more information, see [here](https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc18de5",
   "metadata": {},
   "source": [
    "## Deploy the Model\n",
    "Now that the model is trained, we can deploy this model as a SageMaker Endpoint using the SageMaker hosting services. \n",
    "This is quite easily done using ``model.deploy`` command or via the SageMaker console.\n",
    "\n",
    "This will take a few minutes to execute.\n",
    "\n",
    "Note: SageMaker now supports deploying a [serverless endpoint in preview](https://docs.aws.amazon.com/sagemaker/latest/dg/serverless-endpoints.html) providing a cost effective pay per use cost model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda8b3c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.serializers import JSONSerializer\n",
    "\n",
    "text_classifier = bt_model.deploy(\n",
    "    initial_instance_count=1, instance_type=\"ml.m4.xlarge\", serializer=JSONSerializer(), endpoint_name=\"bt-nlp-endpoint\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be61747",
   "metadata": {},
   "source": [
    "## Test the SageMaker Endpoint\n",
    "Once deployed, the SageMaker Endpoint can be invoked from a Jupyter Notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15475b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    \"I love this product. I would order it again\"\n",
    "]\n",
    "\n",
    "# using the same nltk tokenizer that we used during data preparation for training\n",
    "tokenized_sentences = [\" \".join(nltk.word_tokenize(sent)) for sent in sentences]\n",
    "payload = {\"instances\": tokenized_sentences}\n",
    "print (payload)\n",
    "response = text_classifier.predict(payload)\n",
    "\n",
    "\n",
    "\n",
    "predictions = json.loads(response)\n",
    "print(json.dumps(predictions, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "588caefc",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "Once the endpoint is deployed, it can be invoked by an Lambda function fronted by an API gateway. The API calls to the SageMaker Endpoint can be secured using the AWS security best practices.\n",
    "As part of the LakeHouse formation Immersion day, we will use Athena to call a User Defined Function to invoke a SageMaker Endpoint hosting an ML Model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
